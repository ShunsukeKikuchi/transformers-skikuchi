{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: No generate config is provided, the generate config now is \\{\\}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 9/9 [00:12<00:00,  1.39s/it]\n",
      "Some kwargs in processor config are unused and will not have any effect: protein_max_length, text_max_length. \n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from transformers import EvollaModel, EvollaConfig, EvollaForProteinText2Text\n",
    "# model_feature = EvollaModel.from_pretrained(\"/zhouxibin/workspaces/transformers/evolla-base\")\n",
    "hf_model = transformers.EvollaForProteinText2Text.from_pretrained(\"/zhouxibin/workspaces/transformers/evolla-base\")\n",
    "hf_model = hf_model.to(\"cuda\")\n",
    "\n",
    "hf_processor = transformers.EvollaProcessor.from_pretrained(\"/zhouxibin/workspaces/transformers/evolla-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pl/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generate config is provided, the generate config is:  {'max_new_tokens': 512, 'do_sample': True, 'temperature': 0.6, 'top_p': 0.9}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of EsmForMaskedLM were not initialized from the model checkpoint at /zhouxibin/models/huggingface/SaProt_650M_AF2 and are newly initialized: ['esm.contact_head.regression.bias', 'esm.contact_head.regression.weight', 'esm.embeddings.position_embeddings.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantization is disabled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.65s/it]\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('/zhouxibin/workspaces/Evolla/')\n",
    "from utils.others import load_config, load_model_from_config\n",
    "config_path = \"/zhouxibin/workspaces/Evolla/config/Evolla_10B.yaml\"\n",
    "config = load_config(config_path)\n",
    "model = load_model_from_config(config, local_rank=0, dtype=\"bf16\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import transformers\n",
    "import torch\n",
    "import pickle as pkl\n",
    "uniprot_id = \"C9RH78\"\n",
    "aa_seq = \"MLLEETLKSCPIVKRGKYHYFIHPISDGVPLVEPKLLREVATRIIKIGNFEGVNKIVTAEAMGIPLVTTLSLYTDIPYVIMRKREYKLPGEVPVFQSTGYSKGQLYLNGIEKGDKVIIIDDVISTGGTMIAIINALERAGAEIKDIICVIERGDGKKIVEEKTGYKIKTLVKIDVVDGEVVIL\"\n",
    "foldseek = \"dvvvvqqqpfawdddppdtdgcgclapvpdpddpvvlvvllvlcvvpadpvqaqeeeeeddscpsnvvsncvvpvhyydywylddppdppkdwqwf######gitidpdqaaaheyeyeeaeqdqlrvvlsvvvrcvvrnyhhrayeyaeyhycnqvvccvvpvghyhynwywdqdpsgidtd\"\n",
    "question = \"What is the catalytic activity of this protein?\"\n",
    "hf_inputs = hf_processor(proteins=[{\"aa_seq\": aa_seq, \"foldseek\": foldseek}], messages_list=[[{\"role\": \"system\", \"content\": \"You are an AI expert that can answer any questions about protein.\"}, {\"role\": \"user\", \"content\": question}]]).to(\"cuda\")\n",
    "with torch.no_grad():\n",
    "    hf_output = hf_model.forward(**hf_inputs, output_hidden_states=True)\n",
    "hf_output.hidden_states = [h.to(\"cpu\") for h in hf_output.hidden_states]\n",
    "hf_output.logits = hf_output.logits.to(\"cpu\")\n",
    "hf_output\n",
    "with open(\"hf_hidden_states.pkl\", \"wb\") as f:\n",
    "    pkl.dump(hf_output.hidden_states, f)\n",
    "with open(\"hf_logits.pkl\", \"wb\") as f:\n",
    "    pkl.dump(hf_output.logits, f)\n",
    "# mixed_sequence = \"\".join([a + f for a, f in zip(aa_seq, foldseek)])\n",
    "# outputs = model.forward(seqs=[mixed_sequence], questions=[question], foldseeks=None, answers=None)\n",
    "# outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save ckpt to huggingface format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['module', 'buffer_names', 'optimizer', 'param_shapes', 'frozen_param_shapes', 'shared_params', 'frozen_param_fragments', 'lr_scheduler', 'data_sampler', 'random_ltd', 'sparse_tensor_module_names', 'skipped_steps', 'global_steps', 'global_samples', 'dp_world_size', 'mp_world_size', 'ds_config', 'ds_version', 'epoch', 'global_step', 'pytorch-lightning_version', 'loops', 'callbacks', 'lr_schedulers'])\n"
     ]
    }
   ],
   "source": [
    "# state_dict_path = \"/yuanfajie/ProteinQA/weights/llama3/227_mix_10m_swissprot_zk_data_protrek_12_step=310000-global_datasample=76799744-valid_loss_key=0.59092/epoch=7-step=310000-global_datasample=76799744-valid_loss_key=0.59092.ckpt/checkpoint/mp_rank_00_model_states.pt\"\n",
    "state_dict_path = \"/yuanfajie/zyq-ProteinQA/DPO_ckpt_from_227_step_310000/step=63240-global_datasample=2023680-valid_loss=4.819.ckpt/checkpoint/mp_rank_00_model_states.pt\"\n",
    "import torch\n",
    "state_dict = torch.load(state_dict_path)\n",
    "print(state_dict.keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1055"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_state_dict = model.state_dict()\n",
    "ckpt_state_dict = state_dict['module']\n",
    "len(model_state_dict), len(ckpt_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping: llm.model.lm_head.weight\n",
      "Skipping: protein_encoder.model.esm.contact_head.regression.weight\n",
      "Skipping: protein_encoder.model.esm.contact_head.regression.bias\n",
      "Skipping: protein_encoder.model.lm_head.bias\n",
      "Skipping: protein_encoder.model.lm_head.dense.weight\n",
      "Skipping: protein_encoder.model.lm_head.dense.bias\n",
      "Skipping: protein_encoder.model.lm_head.layer_norm.weight\n",
      "Skipping: protein_encoder.model.lm_head.layer_norm.bias\n",
      "Skipping: protein_encoder.model.lm_head.decoder.weight\n"
     ]
    }
   ],
   "source": [
    "mapping_dict = {\n",
    "    \"llm.model.model\": \"model.llm\",\n",
    "    \"llm.model.lm_head\": \"lm_head\",\n",
    "    # \"llm.model.model.layers\": \"llm.layers\"\n",
    "    \"protein_encoder.model.esm.contact_head\": None,\n",
    "    \"protein_encoder.model.esm\": \"model.protein_encoder.model\",\n",
    "    \"protein_encoder.model.lm_head\": None,\n",
    "    \"protein_encoder.resampler\": \"model.protein_encoder.sequence_compressor_resampler\"\n",
    "}\n",
    "\n",
    "applied_ckpt_state_dict = {}\n",
    "for ckpt_k in ckpt_state_dict.keys():\n",
    "    # Check if any key in mapping_dict is a prefix of ckpt_k\n",
    "    matched = False\n",
    "    for mapping_key in mapping_dict.keys():\n",
    "        if ckpt_k.startswith(mapping_key):\n",
    "            # Use the mapping for the prefix\n",
    "            model_k_start = mapping_dict[mapping_key]\n",
    "            if model_k_start is None:\n",
    "                print(f\"Skipping: {ckpt_k}\")\n",
    "                matched = True\n",
    "                break\n",
    "            model_k = ckpt_k.replace(mapping_key, model_k_start)\n",
    "            try:\n",
    "                assert ckpt_state_dict[ckpt_k].shape == model_state_dict[model_k].shape, f\"Shape mismatch: {ckpt_k} -> {model_k}\"\n",
    "            except KeyError:\n",
    "                print(ckpt_k, model_k, mapping_key)\n",
    "            applied_ckpt_state_dict[model_k] = ckpt_state_dict[ckpt_k]\n",
    "            matched = True\n",
    "            # print(f\"Matched: {ckpt_k} -> {model_k}\")\n",
    "            break\n",
    "    if not matched:\n",
    "        print(f\"No match found for: {ckpt_k}, shape: {ckpt_state_dict[ckpt_k].shape}\")\n",
    "        break  # Break loop if further keys are not processed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(state_dict=applied_ckpt_state_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-02-12 11:09:02,148] [INFO] [real_accelerator.py:203:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/miniconda3/envs/pl/compiler_compat/ld: cannot find -laio: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n",
      "/root/miniconda3/envs/pl/compiler_compat/ld: cannot find -lcufile: No such file or directory\n",
      "collect2: error: ld returned 1 exit status\n"
     ]
    }
   ],
   "source": [
    "model.save_pretrained('evolla-base')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# save and load processor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The tokenizer class you load from this checkpoint is not the same type as the class this function is called from. It may result in unexpected tokenization. \n",
      "The tokenizer class you load from this checkpoint is 'PreTrainedTokenizerFast'. \n",
      "The class this function is called from is 'LlamaTokenizerFast'.\n"
     ]
    }
   ],
   "source": [
    "from transformers import EsmTokenizer, LlamaTokenizerFast, EvollaProcessor\n",
    "import transformers\n",
    "import importlib\n",
    "importlib.reload(transformers)\n",
    "from transformers import EsmTokenizer, LlamaTokenizerFast, EvollaProcessor\n",
    "protein_tokenizer = EsmTokenizer.from_pretrained(\"/zhouxibin/workspaces/ProteinQA/Models/SaProt_35M_AF2\")\n",
    "tokenizer = LlamaTokenizerFast.from_pretrained(\"/zhouxibin/workspaces/ProteinQA/Models/meta-llama_Meta-Llama-3-8B-Instruct\")\n",
    "processor = EvollaProcessor(protein_tokenizer, tokenizer)\n",
    "processor.save_pretrained(\"evolla-base\")\n",
    "processor = EvollaProcessor.from_pretrained(\"evolla-base\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pl",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
