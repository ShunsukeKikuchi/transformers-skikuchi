#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/evev2/modular_evev2.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_evev2.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨

import torch
from torch import nn

from .configuration_evev2 import EVEV2Config


class EVEV2VisionTokenizer(nn.Module):
    def __init__(self, config: EVEV2Config):
        super().__init__()

        input_size = config.mm_hidden_size
        output_size = config.hidden_size
        patch_stride = config.patch_stride
        dense_stride = config.dense_stride

        self.patch_embedding = nn.Sequential(
            nn.Conv2d(3, input_size, kernel_size=patch_stride, stride=patch_stride),
            nn.GELU(),
            nn.Conv2d(input_size, output_size, kernel_size=dense_stride, stride=dense_stride),
        )
        self.class_embedding = nn.Parameter(torch.randn(output_size))
        self.split_embedding = nn.Parameter(torch.randn(output_size))

    def forward(self, pixel_values):
        patch_embeds = []
        for i in range(len(pixel_values)):
            pixel_value = pixel_values[i].to(dtype=self.dtype)
            patch_embed = self.patch_embedding(pixel_value.unsqueeze(0))[0]
            split_embed = self.split_embedding[:, None, None].repeat(1, patch_embed.shape[1], 1)
            patch_embed = torch.cat([patch_embed, split_embed.to(dtype=self.dtype)], dim=-1)

            class_embed = self.class_embedding[None, :].to(dtype=self.dtype)
            patch_embeds.append(torch.cat([class_embed, patch_embed.flatten(1).transpose(0, 1)], dim=0))

        return patch_embeds

    @property
    def dtype(self):
        return self.patch_embedding[0].weight.dtype

    @property
    def device(self):
        return self.patch_embedding[0].weight.device
