#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
#           This file was automatically generated from src/transformers/models/bagel/modular_bagel.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_bagel.py file directly. One of our CI enforces this.
#                ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨ğŸš¨
# coding=utf-8
# Copyright 2025 ByteDance and The HuggingFace Team. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.


from ...configuration_utils import PretrainedConfig
from ...modeling_utils import logging
from ..auto import CONFIG_MAPPING, AutoConfig


logger = logging.get_logger(__name__)


class BagelVQVAEConfig(PretrainedConfig):
    model_type = "bagel_vqvae"
    base_config_key = "vq_config"

    def __init__(
        self,
        double_latent: bool = False,
        latent_channels: int = 256,
        num_patches: int = 32,
        in_channels: int = 3,
        out_channels: int = 3,
        base_channels: int = 128,
        channel_multiplier: list[int] = [1, 2, 4, 4],
        num_res_blocks: int = 2,
        dropout: float = 0.0,
        initializer_range=0.02,
        scale_factor=0.2611,
        shift_factor=0.1159,
        **kwargs,
    ):
        super().__init__(**kwargs)
        self.double_latent = double_latent
        self.latent_channels = latent_channels
        self.in_channels = in_channels
        self.base_channels = base_channels
        self.channel_multiplier = channel_multiplier
        self.num_res_blocks = num_res_blocks
        self.dropout = dropout
        self.initializer_range = initializer_range
        self.num_patches = num_patches
        self.out_channels = out_channels
        self.scale_factor = scale_factor
        self.shift_factor = shift_factor


class BagelConfig(PretrainedConfig):
    model_type = "bagel"
    sub_configs = {
        "text_config": AutoConfig,
        "vision_config": AutoConfig,
        "vq_config": BagelVQVAEConfig,
    }

    def __init__(
        self,
        text_config=None,
        vision_config=None,
        vq_config=None,
        **kwargs,
    ):
        if isinstance(text_config, dict):
            text_config["model_type"] = text_config.get("model_type", "qwen2")
            self.text_config = CONFIG_MAPPING[text_config["model_type"]](**text_config)
        elif text_config is None:
            logger.info("`text_config` is None. Initializing with default values")
            self.text_config = CONFIG_MAPPING["qwen2"]()
        elif isinstance(text_config, PretrainedConfig):
            self.text_config = text_config
        else:
            raise ValueError(
                f"Invalid type for `text_config`. Must be either `dict` or `LlamaConfig`."
                f" Type found: {type(text_config)}"
            )

        if isinstance(text_config, dict):
            vision_config["model_type"] = vision_config.get("model_type", "siglip_vision_model")
            self.vision_config = CONFIG_MAPPING[text_config["model_type"]](**text_config)
        elif text_config is None:
            logger.info("`vision_config` is None. Initializing with default values")
            self.vision_config = CONFIG_MAPPING["siglip_vision_model"]()
        elif isinstance(vision_config, PretrainedConfig):
            self.vision_config = vision_config
        else:
            raise ValueError(
                f"Invalid type for `vision_config`. Must be either `dict` or `JanusVisionConfig`."
                f" Type found: {type(vision_config)}"
            )

        if vq_config is None:
            logger.info("`vq_config` is None. Initializing with default JanusVQVAEConfig values")
            self.vq_config = BagelVQVAEConfig()
        elif isinstance(vq_config, dict):
            self.vq_config = BagelVQVAEConfig(**vq_config)
        elif isinstance(vq_config, BagelVQVAEConfig):
            self.vq_config = vq_config
        else:
            raise ValueError(
                f"Invalid type for `vq_config`. Must be either `dict` or `JanusVQVAEConfig`."
                f" Type found: {type(vq_config)}"
            )

        super().__init__(**kwargs)


__all__ = ["BagelVQVAEConfig", "BagelConfig"]
