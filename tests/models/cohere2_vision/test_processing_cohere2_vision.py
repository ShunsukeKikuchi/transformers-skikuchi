from pathlib import Path

import pytest
import torch
from PIL import Image

from transformers.models.cohere2_vision import Cohere2VisionProcessor


@pytest.fixture
def processor():
    return build_preprocessor()


def build_preprocessor():
    model_id = "CohereLabs/command-a-vision-07-2025"
    processor = Cohere2VisionProcessor.from_pretrained(model_id)
    return processor


@pytest.fixture
def test_data_dir() -> Path:
    return Path(__file__).resolve().parent / "test_data"


@pytest.fixture
def content_turns(test_data_dir):
    return build_content_turns(test_data_dir)


def build_content_turns(test_data_dir):
    jpg_local_path = test_data_dir / "australia.jpg"
    pil_image = Image.open(jpg_local_path).convert("RGB")

    turns = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "And here’s a local image:"},
                {"type": "image", "image": pil_image},
            ],
        },
        {
            "role": "user",
            "content": [{"type": "text", "text": "What did you see in the last image?"}],
        },
    ]
    return turns


@pytest.fixture
def preproc_output_tensors(test_data_dir):
    """
    Load expected outputs of preprocessing from disk.

    These are the expected outputs for content array returned by build_content_turns.
    Data is generated by running python tests/models/cohere2_vision/test_processing_cohere2_vision.py
    (i.e. see the __main__ block).
    """
    interleaved = torch.load(test_data_dir / "preprocessed_inputs_interleaved.pt")
    text_only = torch.load(test_data_dir / "preprocessed_inputs_text_only.pt")
    image_only = torch.load(test_data_dir / "preprocessed_inputs_image_only.pt")
    return {
        "interleaved": interleaved,
        "text_only": text_only,
        "image_only": image_only,
    }


def check_outputs_equal(outputs, expected, tokenizer):
    keys_different = []
    for key, value in outputs.items():
        assert key in expected, f"Key {key} not found in expected outputs"
        if not torch.equal(value, expected[key]):
            keys_different.append(key)

    if "input_ids" in keys_different:
        # error message: decode input_ids to text
        input_ids = outputs["input_ids"]
        expected_input_ids = expected["input_ids"]
        raise build_text_assert_error(input_ids, expected_input_ids, tokenizer)

    assert not keys_different, f"Mismatch for keys: {keys_different}"


def build_text_assert_error(input_ids, expected_input_ids, tokenizer):
    text_inputs = tokenizer.batch_decode(input_ids, skip_special_tokens=False)
    expected_text_inputs = tokenizer.batch_decode(expected_input_ids, skip_special_tokens=False)
    error_str = [
        f"batch {i} OUTPUT:\n{text_inputs[i]}\n\n!=\nbatch {i} EXPECTED:\n{expected_text_inputs[i]}"
        for i in range(len(text_inputs))
    ]

    return AssertionError("Mismatch for input_ids:\n" + "\n".join(error_str))


@pytest.mark.parametrize("sample_type", ["image_only", "text_only", "interleaved"])
def test_cohere2_vision_processor_apply_chat_template(processor, content_turns, preproc_output_tensors, sample_type):
    image_turn, text_turn = content_turns
    if sample_type == "image_only":
        content_turns = [image_turn]
    elif sample_type == "text_only":
        content_turns = [text_turn]

    preproc_outputs = processor.apply_chat_template(
        content_turns,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
        return_dict=True,
    )
    check_outputs_equal(preproc_outputs, preproc_output_tensors[sample_type], processor.tokenizer)


def test_cohere2_vision_processor_apply_chat_template_batched(processor, content_turns, preproc_output_tensors):
    image_turn, text_turn = content_turns
    content_turns_batch = [content_turns, [image_turn], [text_turn]]

    preproc_outputs = processor.apply_chat_template(
        content_turns_batch,
        tokenize=True,
        add_generation_prompt=True,
        return_tensors="pt",
        return_dict=True,
    )

    # check outputs are as expected for cross product of
    # ['input_ids', 'attention_mask', 'pixel_values', 'image_num_patches']
    # x
    # [interleaved, [image_turn], [text_turn]]

    for key in ("input_ids", "attention_mask"):
        for sample_idx, content_type in enumerate(("interleaved", "image_only", "text_only")):
            outputs = preproc_outputs[key][sample_idx]
            expected = preproc_output_tensors[content_type][key].squeeze(0)

            # sometimes there will be padding
            if len(outputs) > len(expected):
                # we use left padding so starts with padding
                pad_end = len(outputs) - len(expected)
                outputs = outputs[pad_end:]

            assert outputs.shape == expected.shape, (
                f"Shape mismatch for key {key} in sample {sample_idx} "
                f"({content_type}): {outputs.shape} != {expected.shape}"
            )
            tensors_eq = torch.equal(outputs, expected)
            if key == "input_ids" and not tensors_eq:
                if not torch.equal(outputs, expected):
                    raise build_text_assert_error([outputs], [expected], processor.tokenizer)
            else:
                assert torch.equal(outputs, expected), (
                    f"Outputs for key {key} in sample {sample_idx} ({content_type}) do not match expected outputs."
                )

    for key in ("pixel_values", "image_num_patches"):
        for sample_idx, content_type in enumerate(("interleaved", "image_only", "text_only")):
            if content_type == "text_only":
                continue
            outputs = preproc_outputs[key][sample_idx]
            expected = preproc_output_tensors[content_type][key].squeeze(0)
            tensors_eq = torch.equal(outputs, expected)

            if not tensors_eq:
                raise AssertionError(
                    f"Outputs for key {key} in sample {sample_idx} ({content_type}) do not match expected outputs."
                )
        if key == "pixel_values":
            pixels_sample1 = preproc_outputs[key][0]
            assert torch.equal(pixels_sample1, preproc_outputs[key][1]), (
                f"Pixel values for sample 0 and 1 should be equal, but are not: "
                f"{pixels_sample1.shape} != {preproc_outputs[key][1].shape}"
            )


def test_cohere2_vision_processor_apply_chat_template_no_tokenize(processor, content_turns, preproc_output_tensors):
    expected_prompt = """<|START_OF_TURN_TOKEN|><|USER_TOKEN|>And here’s a local image:<image><|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|USER_TOKEN|>What did you see in the last image?<|END_OF_TURN_TOKEN|><|START_OF_TURN_TOKEN|><|CHATBOT_TOKEN|>"""
    pil_img = content_turns[0]["content"][1]["image"]
    prompt = processor.apply_chat_template(
        content_turns,
        tokenize=False,
        add_generation_prompt=True,
        return_tensors="pt",
        return_dict=True,
    )
    assert isinstance(prompt, str)
    assert prompt == expected_prompt, f"Prompt mismatch: {prompt} != {expected_prompt}"

    # Note: with tokenize=False you need to call the processor directly:
    preproc_inputs = processor(images=[pil_img], text=prompt, return_tensors="pt")

    check_outputs_equal(preproc_inputs, preproc_output_tensors["interleaved"], processor.tokenizer)


def test_cohere2_vision_processor_decode(processor, preproc_output_tensors):
    """
    Test that the processor can decode the input_ids correctly.
    """
    input_ids = preproc_output_tensors["interleaved"]["input_ids"]
    assert input_ids.ndim == 2, "Input IDs should be 2D tensor"
    input_ids = input_ids[0]  # Take the first batch for decoding
    decoded_text = processor.decode(input_ids, skip_special_tokens=False)

    assert decoded_text.startswith(
        "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>And here’s a local image:<|START_OF_IMG|>"
    )


def test_cohere2_vision_processor_batch_decode(processor, preproc_output_tensors):
    """
    Test that the processor can decode the input_ids correctly.
    """
    input_ids_batched = preproc_output_tensors["interleaved"]["input_ids"]

    decoded_texts = processor.batch_decode(input_ids_batched, skip_special_tokens=False)

    assert decoded_texts[0].startswith(
        "<BOS_TOKEN><|START_OF_TURN_TOKEN|><|USER_TOKEN|>And here’s a local image:<|START_OF_IMG|>"
    )
