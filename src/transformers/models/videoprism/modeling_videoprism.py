#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
#           This file was automatically generated from src/transformers/models/videoprism/modular_videoprism.py.
#               Do NOT edit this file manually as any edits will be overwritten by the generation of
#             the file from the modular. If any change should be done, please apply the change to the
#                          modular_videoprism.py file directly. One of our CI enforces this.
#                ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨ðŸš¨
import math
from dataclasses import dataclass
from typing import Callable, Optional, Union

import torch
import torch.nn as nn
import torch.nn.functional as F

from ...activations import ACT2FN
from ...modeling_layers import GradientCheckpointingLayer
from ...modeling_outputs import BaseModelOutput, BaseModelOutputWithPooling
from ...modeling_utils import ALL_ATTENTION_FUNCTIONS, PreTrainedModel
from ...pytorch_utils import find_pruneable_heads_and_indices, prune_linear_layer
from ...utils import ModelOutput, auto_docstring, logging
from .configuration_videoprism import VideoPrismConfig


logger = logging.get_logger(__name__)


@dataclass
class BaseModelOutputWithSpatialAndTemporalStates(ModelOutput):
    """
    Base class for model outputs with spatial and temporal states.
    """

    last_hidden_state: Optional[torch.FloatTensor] = None
    temporal_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None
    spatial_hidden_states: Optional[tuple[torch.FloatTensor, ...]] = None
    temporal_attentions: Optional[tuple[torch.FloatTensor, ...]] = None
    spatial_attentions: Optional[tuple[torch.FloatTensor, ...]] = None


class VideoPrismTubeletEmbeddings(nn.Module):
    """
    Construct VideoPrism Tubelet embeddings.

    This module turns a batch of videos of shape (batch_size, num_frames, num_channels, height, width) into a tensor of
    shape (batch_size, seq_len, hidden_size) to be consumed by a Transformer encoder.

    The seq_len (the number of patches) equals (number of frames // tubelet_size[0]) * (height // tubelet_size[1]) *
    (width // tubelet_size[2]).
    """

    def __init__(self, config):
        super().__init__()
        self.num_frames = config.num_frames

        self.image_size = (
            config.image_size if isinstance(config.image_size, tuple) else (config.image_size, config.image_size)
        )
        self.patch_size = config.tubelet_size
        self.num_patches = (
            (self.image_size[1] // self.patch_size[2])
            * (self.image_size[0] // self.patch_size[1])
            * (self.num_frames // self.patch_size[0])
        )
        self.embed_dim = config.hidden_size

        self.projection = nn.Conv3d(
            config.num_channels, config.hidden_size, kernel_size=config.tubelet_size, stride=config.tubelet_size
        )

    def forward(self, pixel_values, interpolate_pos_encoding: bool = False, mode="spatial"):
        batch_size, num_frames, num_channels, height, width = pixel_values.shape
        if not interpolate_pos_encoding and (height != self.image_size[0] or width != self.image_size[1]):
            raise ValueError(
                f"Image image size ({height}*{width}) doesn't match model ({self.image_size[0]}*{self.image_size[1]})."
            )

        pixel_values = pixel_values.permute(0, 2, 1, 3, 4)

        x = self.projection(pixel_values)  # ? (B, 768, 16, 16, 16)

        # ? I need to reshape it to (B * T, 256, 768) where 256 is the number of patches and 768 is the embedding dimension

        x = x.flatten(3).permute(0, 2, 3, 1)  # ? (B, T, 256, 768)

        x = x.view(
            x.shape[0] * x.shape[1], x.shape[2], x.shape[3]
        )  # ? (B * T, 256, 768) where 256 is the number of patches and 768 is the embedding dimension

        return x


class VideoPrismEmbeddings(nn.Module):
    """
    VideoPrism Embeddings.

    Creates embeddings from a video using VideoPrismTubeletEmbeddings, adds CLS token and positional embeddings.
    """

    def __init__(self, config: VideoPrismConfig, mode: str = "spatial"):
        super().__init__()
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        self.patch_size = config.tubelet_size[1:]
        self.config = config

        self.mode = mode
        self.tubelet_size = config.tubelet_size
        self.pos_emb_shape = [
            config.num_frames,
            config.image_size // self.patch_size[0],
            config.image_size // self.patch_size[1],
        ]  # ? [16, 16, 16]

        if self.mode == "spatial":
            self.patch_embeddings = VideoPrismTubeletEmbeddings(config)
            self.spatial_pos_emb = nn.Parameter(
                torch.zeros(1, self.pos_emb_shape[1] * self.pos_emb_shape[2], config.hidden_size)
            )  # ? takes in patches of shape (B * T, 256, 768) returns (1, 256, 768) where 256 is the number of patches and 768 is the embedding dimension
        elif self.mode == "temporal":
            self.temporal_pos_emb = nn.Parameter(torch.zeros(1, self.pos_emb_shape[0], config.hidden_size))

    def forward(self, pixel_values: torch.Tensor, input_shape, interpolate_pos_encoding: bool = False):
        if self.mode == "spatial":
            b, t, c, h, w = input_shape
            assert h == w

            embeddings = self.patch_embeddings(pixel_values)

            num_row_patches = h // self.tubelet_size[1]  # ? 288/18 = 16
            num_column_patches = w // self.tubelet_size[2]  # ? 288/18 = 16

            spatial_pos_emb_shape = self.pos_emb_shape[-2:]

            spatial_pos_emb = self.spatial_pos_emb
            if spatial_pos_emb_shape != (num_row_patches, num_column_patches):  # ? got a big issue here
                spatial_pos_emb = self._interpolate_emb_2d(
                    spatial_pos_emb,  # ? 1, 256, 768
                    spatial_pos_emb_shape,
                    (num_row_patches, num_column_patches),
                )
                # raise ValueError(f'Positional embedding should have batch size of 1, got {self.spatial_pos_emb.shape[0]}.')

            embeddings = embeddings + spatial_pos_emb

            return embeddings

        elif self.mode == "temporal":
            if input_shape is not None:
                b, t, c, h, w = input_shape

            _, features, dim = (
                pixel_values.shape
            )  # ? pixel_values has shape (B * T, 256, 768) where 256 is the number of patches and 768 is the embedding dimension

            embeddings = pixel_values.view(b, t, features, dim)  # ? embeddings has shape (B*T, 256, 768)
            embeddings = embeddings.permute(0, 2, 1, 3)
            embeddings = embeddings.view(b * features, t, dim)  # ? embeddings has shape (B * 256, T=16, 768)

            temporal_seq_length = self.pos_emb_shape[0]  # ? 16
            # ? temporal_pos_emb shape is (1, 16, 768)
            temporal_pos_emb = self.temporal_pos_emb
            if temporal_seq_length != t:
                temporal_pos_emb = self._interpolate_emb_1d(self.temporal_pos_emb, t)
                # raise ValueError(f'Positional embedding should have batch size of 1, got {temporal_pos_emb.shape[0]}.') #! to remove
            embeddings = embeddings + temporal_pos_emb  # ? embeddings has shape (B * 256, T=16, 768)
            return embeddings

        else:
            raise ValueError(f"Unknown mode: {self.mode}. Supported modes are: spatial, temporal.")

    def _interpolate_emb_2d(
        self, emb: torch.Tensor, source_emb_shape: tuple[int, int], target_emb_shape: tuple[int, int]
    ):
        # ? emb.shape is (1, 256, 768)
        if len(emb.shape) > 3 or emb.shape[0] != 1:
            raise ValueError("The shape of the embedding should be (1, H * W, D)")

        if emb.shape[-2] != source_emb_shape[0] * source_emb_shape[1]:  # ? 16*16
            raise ValueError("The shape of the embedding does NOT match input specs.")

        emb_dim = emb.shape[-1]
        emb = emb.view(
            emb_dim, source_emb_shape[0], source_emb_shape[1]
        )  # ? 16, 16, 768, the first demsion is remove like squeeze
        emb = emb.unsqueeze(dim=0)
        target_emb = F.interpolate(
            emb,
            (target_emb_shape[0], target_emb_shape[1]),
            mode="bilinear",
            antialias=True,  # ? set to True by default in jax.image.resize
        )

        target_emb = target_emb.view(1, target_emb_shape[0] * target_emb_shape[1], emb_dim)
        return target_emb

    def _interpolate_emb_1d(self, emb: torch.Tensor, target_emb_length: int):
        """
        Interpolates the embedding to the target sequence length
        """
        emb_dim = emb.shape[-1]
        emb = emb.unsqueeze(dim=0)  # jnp.squeeze(emb, axis=0)

        target_emb = F.interpolate(
            emb,  # ? add batch dimension
            (target_emb_length, emb_dim),
            mode="bilinear",
            antialias=True,  # ? set to True by default in jax.image.resize used in the original implementation
        )
        target_emb = target_emb.squeeze(0).view(1, target_emb_length, emb_dim)
        return target_emb


def eager_attention_forward(
    module: nn.Module,
    query: torch.Tensor,
    key: torch.Tensor,
    value: torch.Tensor,
    attention_mask: Optional[torch.Tensor],
    scaling: float,
    dropout: float = 0.0,
    **kwargs,
):
    # Take the dot product between "query" and "key" to get the raw attention scores.
    attn_weights = torch.matmul(query, key.transpose(-1, -2)) * scaling
    # Attention logit capping
    attn_cap = torch.tensor(VideoPrismConfig().atten_logit_cap, dtype=attn_weights.dtype)  #! attention logit capping
    attn_weights = attn_cap * torch.tanh(attn_weights / attn_cap)  #! is only supported in eager mode
    # Normalize the attention scores to probabilities.
    attn_weights = nn.functional.softmax(attn_weights, dim=-1, dtype=torch.float32).to(query.dtype)

    # This is actually dropping out entire tokens to attend to, which might
    # seem a bit unusual, but is taken from the original Transformer paper.
    attn_weights = nn.functional.dropout(attn_weights, p=dropout, training=module.training)

    # Mask heads if we want to
    if attention_mask is not None:
        attn_weights = attn_weights * attention_mask

    attn_output = torch.matmul(attn_weights, value)
    attn_output = attn_output.transpose(1, 2).contiguous()

    return attn_output, attn_weights


class VideoPrismSelfAttention(nn.Module):
    def __init__(self, config: VideoPrismConfig) -> None:
        super().__init__()
        if config.hidden_size % config.num_attention_heads != 0 and not hasattr(config, "embedding_size"):
            raise ValueError(
                f"The hidden size {config.hidden_size} is not a multiple of the number of attention "
                f"heads {config.num_attention_heads}."
            )

        self.config = config
        self.num_attention_heads = config.num_attention_heads
        self.attention_head_size = int(config.hidden_size / config.num_attention_heads)
        self.all_head_size = self.num_attention_heads * self.attention_head_size
        self.dropout_prob = config.attention_probs_dropout_prob
        self.scaling = self.attention_head_size**-0.5
        self.is_causal = False

        self.query = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)
        self.key = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)
        self.value = nn.Linear(config.hidden_size, self.all_head_size, bias=config.qkv_bias)

    def forward(
        self,
        hidden_states,
        head_mask: Optional[torch.Tensor] = None,
        output_attentions: bool = False,
    ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:
        batch_size, seq_length, _ = hidden_states.shape
        key_layer = (
            self.key(hidden_states)
            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)
            .transpose(1, 2)
        )
        value_layer = (
            self.value(hidden_states)
            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)
            .transpose(1, 2)
        )
        query_layer = (
            self.query(hidden_states)
            .view(batch_size, -1, self.num_attention_heads, self.attention_head_size)
            .transpose(1, 2)
        )

        attention_interface: Callable = eager_attention_forward
        if self.config._attn_implementation != "eager":
            if self.config._attn_implementation == "sdpa" and output_attentions:
                logger.warning_once(
                    "`torch.nn.functional.scaled_dot_product_attention` does not support `output_attentions=True`. Falling back to "
                    'eager attention. This warning can be removed using the argument `attn_implementation="eager"` when loading the model.'
                )
            else:
                attention_interface = ALL_ATTENTION_FUNCTIONS[self.config._attn_implementation]

        context_layer, attention_probs = attention_interface(
            self,
            query_layer,
            key_layer,
            value_layer,
            head_mask,
            is_causal=self.is_causal,
            scaling=self.scaling,
            dropout=0.0 if not self.training else self.dropout_prob,
        )

        new_context_layer_shape = context_layer.size()[:-2] + (self.all_head_size,)
        context_layer = context_layer.reshape(new_context_layer_shape)

        outputs = (context_layer, attention_probs) if output_attentions else (context_layer,)

        return outputs


class VideoPrismSelfOutput(nn.Module):
    """
    The residual connection is defined in VideoPrismLayer instead of here (as is the case with other models), due to the
    layernorm applied before each block.
    """

    def __init__(self, config: VideoPrismConfig) -> None:
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.hidden_size)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states: torch.Tensor, input_tensor: torch.Tensor) -> torch.Tensor:
        hidden_states = self.dense(hidden_states)
        hidden_states = self.dropout(hidden_states)

        return hidden_states


class VideoPrismAttention(nn.Module):
    def __init__(self, config: VideoPrismConfig) -> None:
        super().__init__()
        self.attention = VideoPrismSelfAttention(config)
        self.output = VideoPrismSelfOutput(config)
        self.pruned_heads = set()

    def prune_heads(self, heads: set[int]) -> None:
        if len(heads) == 0:
            return
        heads, index = find_pruneable_heads_and_indices(
            heads, self.attention.num_attention_heads, self.attention.attention_head_size, self.pruned_heads
        )

        # Prune linear layers
        self.attention.query = prune_linear_layer(self.attention.query, index)
        self.attention.key = prune_linear_layer(self.attention.key, index)
        self.attention.value = prune_linear_layer(self.attention.value, index)
        self.output.dense = prune_linear_layer(self.output.dense, index, dim=1)

        # Update hyper params and store pruned heads
        self.attention.num_attention_heads = self.attention.num_attention_heads - len(heads)
        self.attention.all_head_size = self.attention.attention_head_size * self.attention.num_attention_heads
        self.pruned_heads = self.pruned_heads.union(heads)

    def forward(
        self,
        hidden_states: torch.Tensor,
        head_mask: Optional[torch.Tensor] = None,
        output_attentions: bool = False,
    ) -> Union[tuple[torch.Tensor, torch.Tensor], tuple[torch.Tensor]]:
        self_outputs = self.attention(hidden_states, head_mask, output_attentions)

        attention_output = self.output(self_outputs[0], hidden_states)

        outputs = (attention_output,) + self_outputs[1:]  # add attentions if we output them
        return outputs


class VideoPrismIntermediate(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.hidden_size, config.intermediate_size)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)
        if isinstance(config.hidden_act, str):
            self.intermediate_act_fn = ACT2FN[config.hidden_act]
        else:
            self.intermediate_act_fn = config.hidden_act

    def forward(self, hidden_states):
        hidden_states = self.dense(hidden_states)
        hidden_states = self.intermediate_act_fn(hidden_states)
        hidden_states = self.dropout(hidden_states)

        return hidden_states


class VideoPrismOutput(nn.Module):
    def __init__(self, config):
        super().__init__()
        self.dense = nn.Linear(config.intermediate_size, config.hidden_size)
        self.dropout = nn.Dropout(config.hidden_dropout_prob)

    def forward(self, hidden_states, input_tensor):
        hidden_states = self.dense(hidden_states)

        hidden_states = self.dropout(hidden_states)

        hidden_states = hidden_states + input_tensor

        return hidden_states


class VideoPrismLayer(GradientCheckpointingLayer):
    """This corresponds to the EncoderBlock class in the scenic/videoprism implementation."""

    def __init__(self, config):
        super().__init__()
        self.attention = VideoPrismAttention(config)
        self.intermediate = VideoPrismIntermediate(config)
        self.output = VideoPrismOutput(config)
        self.layernorm_before = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)
        self.layernorm_after = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

    def forward(self, hidden_states, head_mask=None, output_attentions=False):
        with torch.no_grad():
            self.layernorm_before.weight += nn.Parameter(torch.ones(768))
            self.layernorm_after.weight += nn.Parameter(torch.ones(768))
        self_attention_outputs = self.attention(
            # in VideoPrism, layernorm is applied before self-attention
            self.layernorm_before(hidden_states),
            head_mask,
            output_attentions=output_attentions,
        )
        attention_output = self_attention_outputs[0]
        # add self attentions if we output attention weights
        outputs = self_attention_outputs[1:]

        # first residual connection
        hidden_states = attention_output + hidden_states

        # in VideoPrism, layernorm is also applied after self-attention
        layer_output = self.layernorm_after(hidden_states)
        layer_output = self.intermediate(layer_output)

        # second residual connection is done here
        layer_output = self.output(layer_output, hidden_states)

        outputs = (layer_output,) + outputs

        return outputs


class VideoPrismEncoder(nn.Module):
    def __init__(self, config: VideoPrismConfig, mode: str = "spatial"):
        super().__init__()
        self.config = config
        self.gradient_checkpointing = False
        if mode == "spatial":
            self.layer = nn.ModuleList([VideoPrismLayer(config) for _ in range(config.num_spatial_layers)])
        elif mode == "temporal":
            self.layer = nn.ModuleList([VideoPrismLayer(config) for _ in range(config.num_temporal_layers)])
        else:
            raise ValueError(f"Unknown mode: {mode}. Supported modes are: spatial, temporal.")

    def forward(
        self,
        hidden_states,
        head_mask=None,
        output_attentions=False,
        output_hidden_states=False,
        return_dict=True,
    ):
        all_hidden_states = () if output_hidden_states else None
        all_self_attentions = () if output_attentions else None

        for i, layer_module in enumerate(self.layer):
            if output_hidden_states:
                all_hidden_states = all_hidden_states + (hidden_states,)

            layer_head_mask = head_mask[i] if head_mask is not None else None

            layer_outputs = layer_module(hidden_states, layer_head_mask, output_attentions)

            hidden_states = layer_outputs[0]

            if output_attentions:
                all_self_attentions = all_self_attentions + (layer_outputs[1],)

        if output_hidden_states:
            all_hidden_states = all_hidden_states + (hidden_states,)

        if not return_dict:
            return tuple(v for v in [hidden_states, all_hidden_states, all_self_attentions] if v is not None)
        return BaseModelOutput(
            last_hidden_state=hidden_states,
            hidden_states=all_hidden_states,
            attentions=all_self_attentions,
        )


def lecun_normal_(tensor):
    fan_in = tensor.size(1)  # For Embedding: (num_embeddings, embedding_dim)
    std = math.sqrt(1.0 / fan_in)
    with torch.no_grad():
        return tensor.normal_(0, std)


@auto_docstring
class VideoPrismPreTrainedModel(PreTrainedModel):
    config: VideoPrismConfig

    base_model_prefix = "videoprism"
    main_input_name = "pixel_values"
    supports_gradient_checkpointing = True
    _no_split_modules = []
    _supports_sdpa = True
    _supports_flash_attn = False
    _supports_flex_attn = False
    _supports_attention_backend = True

    def _init_weights(
        self, module
    ):  # todo this needs the exact initialization as in the original VideoPrism implementation
        """Initialize the weights"""
        if isinstance(module, (nn.Linear, nn.Conv3d)):
            # Slightly different from the TF version which uses truncated_normal for initialization
            # cf https://github.com/pytorch/pytorch/pull/5617
            module.weight.data.normal_(mean=0.0, std=self.config.initializer_range)
            if module.bias is not None:
                module.bias.data.zero_()
        elif isinstance(module, nn.LayerNorm):
            module.bias.data.zero_()
            module.weight.data.fill_(1.0)
        elif isinstance(module, VideoPrismEmbeddings):
            if module.mode == "spatial":
                module.patch_embeddings.projection.weight.data = lecun_normal_(
                    module.patch_embeddings.projection.weight.data
                )
                module.spatial_pos_emb.data.zero_()
            elif module.mode == "temporal":
                module.temporal_pos_emb.data.zero_()


@auto_docstring
class VideoPrismModel(VideoPrismPreTrainedModel):
    def __init__(self, config: VideoPrismConfig):
        super().__init__(config)

        self.config = config

        self.spatial_embeddings = VideoPrismEmbeddings(
            config, mode="spatial"
        )  # ? spatial embeddings, takes in (B, T=16, C=3, H=288, W=288) and returns (B * T, 256, 768) where 256 is the number of patches and 768 is the embedding dimension

        self.layernorm1 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

        self.layernorm2 = nn.LayerNorm(config.hidden_size, eps=config.layer_norm_eps)

        self.temporal_embeddings = VideoPrismEmbeddings(config, mode="temporal")

        self.spatial_encoder = VideoPrismEncoder(config, mode="spatial")

        self.temporal_encoder = VideoPrismEncoder(config, mode="temporal")

        self.post_init()

    @auto_docstring
    def forward(
        self,
        pixel_values: Optional[torch.FloatTensor] = None,
        spatial_head_mask: Optional[torch.FloatTensor] = None,  #! These two
        temporal_head_mask: Optional[torch.FloatTensor] = None,  #! are new additions, needfurther work
        output_attentions: Optional[bool] = None,
        output_hidden_states: Optional[bool] = None,
        interpolate_pos_encoding: bool = False,
        return_dict: Optional[bool] = None,
    ) -> Union[tuple[torch.FloatTensor], BaseModelOutputWithPooling]:
        """
        Forward pass of the VideoPrism model
        """
        output_attentions = output_attentions if output_attentions is not None else self.config.output_attentions
        output_hidden_states = (
            output_hidden_states if output_hidden_states is not None else self.config.output_hidden_states
        )
        return_dict = return_dict if return_dict is not None else self.config.use_return_dict

        if pixel_values is None:
            raise ValueError("You have to specify pixel_values")

        spatial_head_mask = (
            self.get_head_mask(spatial_head_mask, self.config.num_spatial_layers)
            if spatial_head_mask is not None
            else None
        )

        temporal_head_mask = (
            self.get_head_mask(temporal_head_mask, self.config.num_temporal_layers)
            if temporal_head_mask is not None
            else None
        )

        input_shape = pixel_values.shape  # ? (B, T=16, C=3, H=288, W=288)

        spatial_embeds = self.spatial_embeddings(pixel_values, input_shape)  # ? embeds has shape (B * T, 256, 768)

        spatial_encoder_outputs = self.spatial_encoder(
            spatial_embeds,
            head_mask=spatial_head_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )  # ? shape (B * T, 256, 768)

        spatial_sequence_output = spatial_encoder_outputs[0]

        with torch.no_grad():
            self.layernorm1.weight += nn.Parameter(
                torch.ones(768)
            )  #! part of the original implementation, not sure why, could an erorr, but is necessay for matching the logits
        features = self.layernorm1(spatial_sequence_output)  # ? shape (B * T, 256, 768)

        # ? spatial_features = (features,) + spatial_encoder_outputs[1:]  #! need to use

        temporal_embeds = self.temporal_embeddings(features, input_shape)  # ? shape (B * T, 256, 768)

        temporal_encoder_outputs = self.temporal_encoder(
            temporal_embeds,
            head_mask=spatial_head_mask,
            output_attentions=output_attentions,
            output_hidden_states=output_hidden_states,
            return_dict=return_dict,
        )  # ? shape (B * T, 256, 768)

        temporal_sequence_output = temporal_encoder_outputs[0]

        with torch.no_grad():
            self.layernorm2.weight += nn.Parameter(torch.ones(768))

        features = self.layernorm2(temporal_sequence_output)  # ? shape is (256, 16, 768)

        # ? temporal_features = (features,) + temporal_encoder_outputs[1:]

        features = (
            features.view(input_shape[0], -1, *features.shape[1:]).permute(0, 2, 1, 3).contiguous()
        )  # ? reshape to (B, 256, 16, 768) then permute to (B, 16, 256, 768)
        features = features.view(input_shape[0], features.shape[1] * features.shape[2], -1)  # ? (B, 256*16, 768)
        if not return_dict:
            return (
                features,
                temporal_encoder_outputs.hidden_states,
                spatial_encoder_outputs.hidden_states,
                temporal_encoder_outputs.attentions,
                spatial_encoder_outputs.attentions,
            )

        return BaseModelOutputWithSpatialAndTemporalStates(
            last_hidden_state=features,
            temporal_hidden_states=temporal_encoder_outputs.hidden_states,
            spatial_hidden_states=spatial_encoder_outputs.hidden_states,
            temporal_attentions=temporal_encoder_outputs.attentions,
            spatial_attentions=spatial_encoder_outputs.attentions,
        )  # ? returns (B * T, 256, 768) where 256 is the number of patches and 768 is the embedding dimension


__all__ = ["VideoPrismModel", "VideoPrismPreTrainedModel"]
